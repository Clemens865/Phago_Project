A quantum bit, or qubit, is the fundamental unit of quantum information that exploits the principle of superposition to exist in a coherent linear combination of the computational basis states zero and one simultaneously, in contrast to a classical bit which is restricted to a single definite binary value at any given time. Mathematically, the state of a qubit is described by a normalized vector in a two-dimensional complex Hilbert space, written as alpha times ket zero plus beta times ket one, where alpha and beta are complex probability amplitudes satisfying the constraint that the sum of their squared moduli equals one, and the relative phase between these amplitudes encodes quantum information that has no classical analogue. The Bloch sphere provides an intuitive geometric representation of a single qubit state as a point on the surface of a unit sphere, where the north and south poles correspond to the basis states zero and one, the equator represents equal superpositions with varying relative phases, and rotations around the sphere axes correspond to single-qubit unitary operations. Physical implementations of qubits span diverse platforms including superconducting transmon qubits that encode information in the quantized charge states of Josephson junction circuits, trapped ion qubits using the internal electronic states of individual atoms confined in electromagnetic Paul traps, semiconductor spin qubits based on the spin state of single electrons in quantum dots, photonic qubits utilizing polarization or path degrees of freedom of single photons, and topological qubits proposed to exploit non-Abelian anyons in fractional quantum Hall systems. For example, a transmon qubit operating at millikelvin temperatures in a dilution refrigerator can achieve coherence times exceeding several hundred microseconds, enabling the execution of thousands of gate operations before decoherence degrades the quantum state, which represents the critical threshold for implementing quantum error correction protocols.